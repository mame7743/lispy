import unittest

from lispy import run, tokenize

# LISPã‚¤ãƒ³ã‚¿ãƒ—ãƒªã‚¿ã®ä¸»è¦ãªé–¢æ•°ï¼ˆå¾Œã§å®Ÿè£…ï¼‰
# parse(code) ã¯æ–‡å­—åˆ—ã®ã‚³ãƒ¼ãƒ‰ã‚’Så¼ã«å¤‰æ›ã™ã‚‹
# evaluate(s_expression) ã¯Så¼ã‚’è©•ä¾¡ã™ã‚‹
# run(code) ã¯ã‚³ãƒ¼ãƒ‰æ–‡å­—åˆ—å…¨ä½“ã‚’å®Ÿè¡Œã™ã‚‹

class TestLispInterpreter(unittest.TestCase):

    def test_add_expression(self):
        """
        ãƒ†ã‚¹ãƒˆ: åŸºæœ¬çš„ãªè¶³ã—ç®—ã®Så¼ã‚’è©•ä¾¡ã§ãã‚‹ã‹
        """
        # (1) æº–å‚™
        code = '(+ 1 2)'

        # (2) å®Ÿè¡Œ
        result = run(code)

        # (3) æ¤œè¨¼
        self.assertEqual(result, 3)

    def test_nested_add_expression(self):
        """
        ãƒ†ã‚¹ãƒˆ: æ‹¬å¼§ã®å…¥ã‚Œå­ã«ãªã£ãŸè¶³ã—ç®—ã‚’è©•ä¾¡ã§ãã‚‹ã‹
        """
        # (1) æº–å‚™
        code = '(+ 1 (+ 2 3))'

        # (2) å®Ÿè¡Œ
        result = run(code)

        # (3) æ¤œè¨¼
        self.assertEqual(result, 6)

    def test_single_number(self):
        """
        ãƒ†ã‚¹ãƒˆ: å˜ä¸€ã®æ•°å€¤ãƒªãƒ†ãƒ©ãƒ«ã‚’è©•ä¾¡ã§ãã‚‹ã‹
        """
        # (1) æº–å‚™
        code = '42'

        # (2) å®Ÿè¡Œ
        result = run(code)

        # (3) æ¤œè¨¼
        self.assertEqual(result, 42)


class TestTokenizer(unittest.TestCase):
    """Tokenizerã®å‹•ä½œã‚’ãƒ†ã‚¹ãƒˆã™ã‚‹ã‚¯ãƒ©ã‚¹"""

    def test_simple_expression_tokenization(self):
        """
        ãƒ†ã‚¹ãƒˆ: å˜ç´”ãªå¼ '(+ 1 2)' ãŒæ­£ã—ããƒˆãƒ¼ã‚¯ãƒ³åŒ–ã•ã‚Œã‚‹ã‹
        """
        # (1) æº–å‚™
        code = '(+ 1 2)'
        expected_tokens = [
            ('LPAREN', '('),
            ('OPERATOR', '+'),
            ('INTEGER', '1'),
            ('INTEGER', '2'),
            ('RPAREN', ')')
        ]

        # (2) å®Ÿè¡Œ
        result = tokenize(code)

        # (3) æ¤œè¨¼
        self.assertEqual(result, expected_tokens)

    def test_nested_expression_tokenization(self):
        """
        ãƒ†ã‚¹ãƒˆ: ãƒã‚¹ãƒˆã—ãŸå¼ '(+ 1 (+ 2 3))' ãŒæ­£ã—ããƒˆãƒ¼ã‚¯ãƒ³åŒ–ã•ã‚Œã‚‹ã‹
        """
        # (1) æº–å‚™
        code = '(+ 1 (+ 2 3))'
        expected_tokens = [
            ('LPAREN', '('),
            ('OPERATOR', '+'),
            ('INTEGER', '1'),
            ('LPAREN', '('),
            ('OPERATOR', '+'),
            ('INTEGER', '2'),
            ('INTEGER', '3'),
            ('RPAREN', ')'),
            ('RPAREN', ')')
        ]

        # (2) å®Ÿè¡Œ
        result = tokenize(code)

        # (3) æ¤œè¨¼
        self.assertEqual(result, expected_tokens)

    def test_number_tokenization(self):
        """
        ãƒ†ã‚¹ãƒˆ: æ•°å€¤ã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ãŒæ­£ã—ãå‹•ä½œã™ã‚‹ã‹
        """
        # (1) æº–å‚™
        code = '42'
        expected_tokens = [('INTEGER', '42')]

        # (2) å®Ÿè¡Œ
        result = tokenize(code)

        # (3) æ¤œè¨¼
        self.assertEqual(result, expected_tokens)

    def test_operator_tokenization(self):
        """
        ãƒ†ã‚¹ãƒˆ: æ¼”ç®—å­ã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ãŒæ­£ã—ãå‹•ä½œã™ã‚‹ã‹
        """
        # (1) æº–å‚™
        test_cases = [
            ('+', [('OPERATOR', '+')]),
            ('-', [('OPERATOR', '-')]),
            ('*', [('OPERATOR', '*')]),
            ('/', [('OPERATOR', '/')]),
        ]

        for code, expected_tokens in test_cases:
            with self.subTest(code=code):
                # (2) å®Ÿè¡Œ
                result = tokenize(code)

                # (3) æ¤œè¨¼
                self.assertEqual(result, expected_tokens)

    def test_symbol_tokenization(self):
        """
        ãƒ†ã‚¹ãƒˆ: ã‚·ãƒ³ãƒœãƒ«ã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ãŒæ­£ã—ãå‹•ä½œã™ã‚‹ã‹
        """
        # (1) æº–å‚™
        test_cases = [
            ('define', [('SYMBOL', 'define')]),
            ('my_var', [('SYMBOL', 'my_var')]),
            ('foo123', [('SYMBOL', 'foo123')]),
        ]

        for code, expected_tokens in test_cases:
            with self.subTest(code=code):
                # (2) å®Ÿè¡Œ
                result = tokenize(code)

                # (3) æ¤œè¨¼
                self.assertEqual(result, expected_tokens)

    def test_whitespace_handling(self):
        """
        ãƒ†ã‚¹ãƒˆ: ç©ºç™½æ–‡å­—ãŒé©åˆ‡ã«å‡¦ç†ã•ã‚Œã‚‹ã‹ï¼ˆç„¡è¦–ã•ã‚Œã‚‹ã‹ï¼‰
        """
        # (1) æº–å‚™
        code = '  ( +   1    2  )  '
        expected_tokens = [
            ('LPAREN', '('),
            ('OPERATOR', '+'),
            ('INTEGER', '1'),
            ('INTEGER', '2'),
            ('RPAREN', ')')
        ]

        # (2) å®Ÿè¡Œ
        result = tokenize(code)

        # (3) æ¤œè¨¼
        self.assertEqual(result, expected_tokens)

    def test_complex_expression_tokenization(self):
        """
        ãƒ†ã‚¹ãƒˆ: ã‚ˆã‚Šè¤‡é›‘ãªå¼ã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
        """
        # (1) æº–å‚™
        code = '(define square (lambda (x) (* x x)))'
        expected_tokens = [
            ('LPAREN', '('),
            ('SYMBOL', 'define'),
            ('SYMBOL', 'square'),
            ('LPAREN', '('),
            ('SYMBOL', 'lambda'),
            ('LPAREN', '('),
            ('SYMBOL', 'x'),
            ('RPAREN', ')'),
            ('LPAREN', '('),
            ('OPERATOR', '*'),
            ('SYMBOL', 'x'),
            ('SYMBOL', 'x'),
            ('RPAREN', ')'),
            ('RPAREN', ')'),
            ('RPAREN', ')')
        ]

        # (2) å®Ÿè¡Œ
        result = tokenize(code)

        # (3) æ¤œè¨¼
        self.assertEqual(result, expected_tokens)

    def test_empty_string_tokenization(self):
        """
        ãƒ†ã‚¹ãƒˆ: ç©ºæ–‡å­—åˆ—ã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
        """
        # (1) æº–å‚™
        code = ''
        expected_tokens = []

        # (2) å®Ÿè¡Œ
        result = tokenize(code)

        # (3) æ¤œè¨¼
        self.assertEqual(result, expected_tokens)


def main():
    """ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã™ã‚‹ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    print("ğŸ§ª Running lispy tests...")
    unittest.main(verbosity=2, exit=False)


if __name__ == '__main__':
    main()